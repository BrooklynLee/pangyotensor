{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Chapter 5. 오차역전파법 (Backpropagation)  \n",
    "\n",
    "* 수치 미분의 장점과 단점\n",
    "    * 장점 - 단순하고 구현이 간단.\n",
    "    * 단점 - 시간이 오래 걸림.\n",
    "        * 가중치 매개변수의 기울기를 효율적으로 계산하기 위해 Backpropagation 도입.\n",
    "\n",
    "\n",
    "* Backpropagation의 이해\n",
    "    * 수식 - 설명 안함.\n",
    "    * 계산 그래프(시각적) - CS231n 참고"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## 5.1 계산 그래프 (Computational Graph)  \n",
    "\n",
    "* 계산 그래프 - 계산 과정을 그래프로 표현. (TensorFlow와 비슷한 접근).\n",
    "    * 노드(Node) - 원(O)으로 표현 - 연산\n",
    "    * 에지(Edge) - 화살표 - 연산의 흐름"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 5.1.1 계산 그래프로 풀다  \n",
    "\n",
    "*문제 1 : 현빈 군은 슈퍼에서 1개에 100원인 사과 2개를 샀습니다. 이때 지불금액을 구하세요. 단 소비세가 10% 부과됩니다.*  \n",
    "(문제 1은 곱으로만 표현 가능)  \n",
    "\n",
    "<center>**Figure 5-1 계산 그래프로 풀어본 문제 1의 답**</center>\n",
    "![Figure_5-01](./images/Figure_5-01.png)  \n",
    "\n",
    "**Figure 5-1**은 상수와 연산이 완전히 분리되어 있지 않으므로 상수와 연산자를 완전히 분리하면 **Figure 5-2**와 같이 표현됨.  \n",
    "(이후에는 **Figure 5-2**와 같은 형식으로 표현)  \n",
    "\n",
    "<center>**Figure 5-2 계산 그래프로 풀어본 문제 1의 답 : '사과의 개수'와 '소비세'를 변수로 취급해 원 밖에 표기**</center>\n",
    "![Figure_5-02](./images/Figure_5-02.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "*문제 2 : 현빈 군은 사과를 2개, 귤을 3개 샀습니다. 사과는 1개에 100원, 귤은 1개 150원입니다. 소비자세가 10%일 때 지불 금액을 구하세요.*  \n",
    "(문제 2는 곱과 합으로 표현 가능)\n",
    "\n",
    "<center>**Figure 5-3 계산 그래프로 풀어본 문제2의 답**</center>\n",
    "![Figure_5-03](./images/Figure_5-03.png)\n",
    "\n",
    "* 계산 그래프를 이용한 문제풀이의 흐름\n",
    "    1. 계산 그래프를 구성한다.\n",
    "    2. 그래프에서 계산을 왼쪽에서 오른쪽으로 진행한다.\n",
    "\n",
    "2와 같이 왼쪽에서 오른쪽으로 진행하는 단계를 순전파 (Forward Propagation).  \n",
    "반대로 오른쪽에서 왼쪽으로 진행하는 단계를 역전파 (Backpropagation)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 5.1.2 국소적 계산  \n",
    "\n",
    "계산 그래프의 특징은 **국소적 계산**을 전파함으로써 최종 결과를 얻는 것.  \n",
    "**국소적이란** 자신과 직접 관계된 작은 범위.  \n",
    "(전체와는 상관없이 자신과 관계된 정보만으로 다음 결과를 출력할 수 있음.)\n",
    "\n",
    "<center>**Figure 5-4 사과 2개를 포함해 여러 식품을 구입하는 예**</center>  \n",
    "![Figure_5-04](./images/Figure_5-04.png)\n",
    "\n",
    "**Figure 5-4**에서 여러 식품을 구입하여 총금액이 4,000원.  \n",
    "이후 사과를 추가로 더 구매한다면 두 값을 더하는 계산(4,000 + 200 -> 4,200)은 4,000이라는 숫자가 어떻게 계산되어 나왔느냐와 관계가 없음.  \n",
    "결국 각 노드는 자신과 관련한 계산(Figure 5-4에서는 두 숫자의 덧셈) 외에는 신경쓸 사항이 없음.  \n",
    "\n",
    "\n",
    "위에서 처럼 계산 그래프는 국소적 계산에만 집중하면 됨.  \n",
    "**간단한 국소적인 계산의 조합으로 복잡한 계산 결과를 얻을 수 있음.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 5.1.3 왜 계산 그래프로 푸는가?  \n",
    "\n",
    "* 계산 그래프의 이점\n",
    "    * 국소적 계산\n",
    "        * 각 노드의 단순한 계산으로 문제를 단순화 할 수 있음.\n",
    "        * 각 노드의 중간 계산 결과를 모두 보관 가능.\n",
    "    * **역전파를 통해 *미분*을 효율적으로 계산 가능.**\n",
    "\n",
    "\n",
    "역전파 설명을 위해 문제 1로 다시 돌아감.  \n",
    "*문제 1 : 현빈 군은 슈퍼에서 1개에 100원인 사과 2개를 샀습니다. 이때 지불금액을 구하세요. 단 소비세가 10% 부과됩니다.*  \n",
    "\n",
    "*문제 1-1 : 사과 가격이 오르면 최종 금액에 어떤 영향을 끼치는지 알고 싶음.*  \n",
    "위 문제는 **사과 가격에 대한 지불 금액의 미분**을 구하는 문제임.\n",
    "\n",
    "사과 값을 $x$, 지불 금액을 $L$이라 했을 때 $\\frac{\\partial L}{\\partial x}$을 구하는 문제.  \n",
    "(사과 값이 **아주 조금** 올랐을 때 지불 금액이 얼마나 증가하느냐?)\n",
    "\n",
    "**사과 가격에 대한 지불 금액의 미분**은 역전파를 하면 구할 수 있음.  \n",
    "**Figure 5-5**는 계산 그래프 상의 역전파를 미분을 통혜서 구해 놓은 결과.(자세한 내용은 뒤에서)  \n",
    "\n",
    "<center>**Figure 5-5 역전파에 의한 미분 값의 전달**</center>\n",
    "![Figure_5-05](./images/Figure_5-05.png)\n",
    "\n",
    "역전파는 순전파와는 반대 방향의 화살표(붉은 선)으로 표시.  \n",
    "이 전파는 **국소적 미분**을 전달하고 그 미분 값을 화살표의 아래에 표시.  \n",
    "이 예에서 역전파는 오른쪽에서 왼쪽으로 **1 -> 1.1 -> 2.2** 순으로 미분 값을 전달.  \n",
    "이 결과로부터 **사과 가격에 대한 지불 금액의 미분**값은 2.2라는 것을 알 수 있음.  \n",
    "사과가 1원 오르면 최종금액은 2.2원 오름.  \n",
    "(사과 가격이 $x$만큼 오르면 최종금액은 2.2$x$만큼 오름.)  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## 5.2 연쇄법칙 (Chain Rule)  \n",
    "\n",
    "**Figure 5-5**처럼 오른쪽에서 왼쪽으로 **국소적 미분**을 전달하는 원리는 **연쇄법칙(Chain Rule)**에 따른 것.  \n",
    "이 절에서 연쇄법칙을 설명하고 그것이 그래프 상의 역전파와 같다는 사실을 확인."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 5.2.1 계산 그래프의 역전파  \n",
    "\n",
    "**Figure 5-6**에서 $y=f(x)$라는 계산의 역전파를 표현.  \n",
    "\n",
    "<center>**Figure 5-6 계산 그래프의 역전파 : 순방향과는 반대 방향으로 국소적 미분을 곱한다.**</center>  \n",
    "![Figure_5-06](./images/Figure_5-06.png)\n",
    "\n",
    "**Figure 5-6**과 같이 역전파의 계산 절차는 신호 $E$에 노드의 국소적 미분$(\\frac{\\partial y}{\\partial x})$를 곱한 후 다음 노드로 전달하는 것.  \n",
    "1. **국소적 미분**은 순전파 때의  $y=f(x)$ 계산의 미분을 구한다는 것이며, 이는 $x$에 대한 $y$의 미분$(\\frac{\\partial y}{\\partial x})$을 구한다는 뜻.  \n",
    "(예 : $y=f(x)=x^2$이라면 $(\\frac{\\partial y}{\\partial x}) = 2x$)\n",
    "2. 그리고 이 국소적인 미분을 상류에서 전달된 값(이 예에서는 E)에 곱해 앞으로 전달하는 것.  \n",
    "\n",
    "위의 방식을 따르면 목표로 하는 미분 값을 효율적으로 구할 수 있다는 것이 역전파의 핵심."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 5.2.2 연쇄법칙이란?  \n",
    "\n",
    "**합성 함수** - 여려 함수로 구성된 함수.  \n",
    "\n",
    "예) $x = {(x + y)}^2$는 **Equation 5.1**처럼 두 개의 식으로 구성.  \n",
    "$$ \n",
    "\\mathbf{Equation \\ 5.1} \\\\ \n",
    "z = t^2 \\\\ \n",
    "t = x + y \\\\\n",
    "$$\n",
    "\n",
    "* 연쇄법칙\n",
    "    * *합성 함수의 미분은 합성 함수를 구성하는 각 함수의 미분의 곱으로 나타낼 수 있다.*  \n",
    "\n",
    "**Equation 5.1**을 예로 설명, $\\frac{\\partial z}{\\partial x}$ ($x$에 대한 $z$의 미분)은  $\\frac{\\partial z}{\\partial t}$  ($t$에 대한 $z$의 미분)과  $\\frac{\\partial t}{\\partial x}$($x$에 대한 $t$의 미분)의 곱으로 나타낼 수 있음.  \n",
    "수식으로는 **Equation 5.2**처럼 표기.  \n",
    "\n",
    "$$\n",
    "\\mathbf{Equation \\ 5.2} \\\\\n",
    "\\frac{\\partial z}{\\partial x} = \\frac{\\partial z}{\\partial t} \\frac{\\partial t}{\\partial x} \n",
    "$$  \n",
    "\n",
    "**Equation 5.2**는 다음과 같이 $\\partial t$를 서로 지울 수 있음.\n",
    "$$\n",
    "\\require{cancel}\n",
    "\\frac{\\partial z}{\\partial x} = \\frac{\\partial z}{\\cancel{\\partial t}} \\frac{\\cancel{\\partial t}}{\\partial x}\n",
    "$$  \n",
    "\n",
    "연쇄 법칙을 써서 **Equation 5.2**의 미분 $\\frac{\\partial z}{\\partial x}$를 진행. 먼저 **Equation 5.1**의 국소적 미분(편미분)을 구함.  \n",
    "$$ \n",
    "\\mathbf{Equation \\ 5.3} \\\\\n",
    "\\frac{\\partial z}{\\partial t} = 2t \\\\ \n",
    "\\frac{\\partial t}{\\partial x} = 1\n",
    "$$  \n",
    "\n",
    "**Equation 5.3**과 같이 $\\frac{\\partial z}{\\partial t}$는 $2t$이고, $\\frac{\\partial t}{\\partial x}$는 1.(미분 공식에서 해석적으로 구한 결과)  \n",
    "최종적으로 구하고자 하는 $\\frac{\\partial z}{\\partial x}$는 **Equation 5.3**에서 구한 두 미분을 곱해서 계산.  \n",
    "\n",
    "$$\n",
    "\\mathbf{Equation \\ 5.4} \\\\\n",
    "\\frac{\\partial z}{\\partial x} = \\frac{\\partial z}{\\partial t} \\frac{\\partial t}{\\partial x} = 2t \\centerdot 1 = 2(x + y)\n",
    "$$  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 5.2.3 연쇄법칙과 계산 그래프  \n",
    "\n",
    "**Equation 5.4**의 연쇄법칙 계산을 계산 그래프로 표현하면 **Figure 5-7**처럼 표현 가능.  \n",
    "(2제곱 계산은 '**$\\ast\\ast2$**'노드로 표현)  \n",
    "\n",
    "<center>**Figure 5-7 *Equation 5.4*의 계산 그래프 : 순전파와는 반대 방향으로 국소적 미분을 곱하여 전달한다.**</center>\n",
    "![Figure_5-07](./images/Figure_5-07.png)  \n",
    "\n",
    "**Figure 5-7**과 같이 계산 그래프의 역전파는 오른쪽에서 왼쪽으로 신호를 전파.  \n",
    "역전파의 계산 절차에서는 노드로 들어온 입력 신호에 그 노드의 국소적 미분(편미분)을 곱한 후 다음 노드로 전달.  \n",
    "\n",
    "1. 역전파의 첫 신호인 $\\frac{\\partial z}{\\partial z}$는 1.\n",
    "2. '$\\ast\\ast2$'노드에서는 입력은 $\\frac{\\partial z}{\\partial z}$에 국소적인 미분 $\\frac{\\partial z}{\\partial t}$를 곱하고 다음 노드로 전달.\n",
    "    * 순전파시 입력이 t이고 출력이 z이므로 이 노드의 (국소적) 미분은 $\\frac{\\partial z}{\\partial t}$.\n",
    "3. +노드에서는 입력이 $\\frac{\\partial z}{\\partial z} \\frac{\\partial z}{\\partial t}$이고 국소적인 미분 $\\frac{\\partial t}{\\partial x}$를 곱하여 다음 노드로 전달하면 $\\frac{\\partial z}{\\partial z} \\frac{\\partial z}{\\partial t} \\frac{\\partial t}{\\partial x}$가 됨.\n",
    "    * $\\frac{\\partial z}{\\partial z} \\frac{\\partial z}{\\partial t} \\frac{\\partial t}{\\partial x} = \\frac{\\partial z}{\\cancel{\\partial z}} \\frac{\\cancel{\\partial z}}{\\cancel{\\partial t}} \\frac{\\cancel{\\partial t}}{\\partial x} = \\frac{\\partial z}{\\partial x}$가 성립되어 **$x$에 대한 $z$의 미분**이 됨.(즉, 역전파가 하는 일은 연쇄 법칙의 원리와 같음.)\n",
    "\n",
    "<center>**Figure 5-8 계산 그래프의 역전파 결과에 따르면 $\\frac{\\partial z}{\\partial x}$는 $2(x + y)$가 된다.**</center>\n",
    "![Figure_5-08](./images/Figure_5-08.png)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 5.3.1 덧셈 노드의 역전파  \n",
    "\n",
    "식 $z = x + y$를 대상으로 역전파 설명.  \n",
    "\n",
    "$z = x + y$의 편미분은 **Equation 5.5**와 같음.\n",
    "\n",
    "$$\n",
    "\\mathbf{Equation \\ 5.5} \\\\  \n",
    "\\frac{\\partial z}{\\partial x} = 1 \\\\  \n",
    "\\frac{\\partial z}{\\partial y} = 1\n",
    "$$  \n",
    "\n",
    "**Equation 5.5**를 **Figure 5-9**와 같이 나타낼 수 있음.\n",
    "\n",
    "<center>**Figure 5-9 덧셈 노드의 역전파 : 왼쪽이 순전파. 오른쪽이 역전파다. 덧셈 노드의 역전파는 입력 값을 그대로 흘려보낸다.**</center>\n",
    "![Figure_5-09](./images/Figure_5-09.png)  \n",
    "\n",
    "**Figure 5-9**와 같이 덧셈 노드의 역전파는 1을 곱하기만 할 뿐이므로 입력된 값을 그대로 다음 노드로 전달.  \n",
    "\n",
    "**Figure 5-9**에서는 상류에서 전해진 미분 값을 $\\frac{\\partial L}{\\partial z}$라고 했는데, 이는 **Figure 5-10**에서와 같이 최종적으로 L이라는 값을 출력하는 큰 계산 그래프를 가정하기 때문.  \n",
    "$z = x + y$ 계산은 그 근 계산 그래프의 중간 어딘가에 존재하고, 상류로부터 $\\frac{\\partial L}{\\partial z}$값이 전해저 내려온 것이고 다시 하류로 $\\frac{\\partial L}{\\partial x} (= \\frac{\\partial L}{\\partial z} \\frac{\\partial z}{\\partial x} )$ 과 $\\frac{\\partial L}{\\partial y} (= \\frac{\\partial L}{\\partial z} \\frac{\\partial z}{\\partial y})$ 값을 전달 하는 것임.  \n",
    "\n",
    "<center>**Figure 5-10 최종 출력으로 가는 계산의 중간에 덧셈 노드가 존재한다. 역전파에서는 국소적 미분이 가장 오른쪽의 출력에서  \n",
    "시작하여 노드를 타고 역방향(왼쪽)으로 전파된다.**</center>\n",
    "![Figure_5-10](./images/Figure_5-10.png)  \n",
    "\n",
    "**Figure 5-11**에서는 **$10 + 5 = 15$**라는 순전파에 대한 식이 주어 지고, 상류에서 1.3이라는 값이 전해진다고 가정함.  \n",
    "덧셈의 역전파는 입력 신호를 다음노드로 그대로 출력할 뿐이므로 1.3을 다음노드로 전달함.  \n",
    "\n",
    "<center>**Figure 5-11 덧셈노드 역전파의 구체적인 예**</center>\n",
    "![Figure_5-11](./images/Figure_5-11.png)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 5.3.2 곱셈 노드의 역전파  \n",
    "\n",
    "식 $z = xy$라는 식으로 역전파 설명.  \n",
    "\n",
    "이 식의 미분은 **Equation 5.6**과 같음.  \n",
    "\n",
    "$$\n",
    "\\mathbf{Equation \\ 5.6} \\\\  \n",
    "\\frac{\\partial z}{\\partial x} = y \\\\  \n",
    "\\frac{\\partial z}{\\partial y} = x\n",
    "$$  \n",
    "\n",
    "**Equation 5.6**을 계산 그래프로 표현하면 **Figure 5-12**와 같음.  \n",
    "\n",
    "<center>**Figure 5-12 곱셈 노드의 역전파 : 왼쪽이 순전파. 오른쪽이 역전파다.**</center>\n",
    "![Figure_5-12](./images/Figure_5-12.png)  \n",
    "\n",
    "**Figure  5-12**와 같이 곱셈 노드 역전파는 상류 값에 순전파 때의 입력 신호들을 **서로 바꾼 값**을 곱해서 하류로 전달.  \n",
    "\n",
    "**Figure 5-13**에서는 **$10 \\times 5 = 50$**이라는 순전파에 대한 식이 주어지고, 역전파 때 상류에서 1.3이 전해진다고 가정함.  \n",
    "곱셈의 역전파는 입력 신호를 바꾼 값을 곱하여 전달하므로 10이 전해졌던 쪽으로는 $6.5(= 1.3 \\times 5)$이 흐르고, 5가 전해졌던 쪽으로는 $13 (= 1.3 \\times 10)$이 흐름.  \n",
    "\n",
    "<center>**Figure 5-13 곱셈 노드 역전파의 구체적인 예**</center>\n",
    "![Figure_5-13](./images/Figure_5-13.png)  \n",
    "\n",
    "**덧셈의 역전파는 순방향의 신호 값을 저장할 필요가 없으나, 곱셈의 역전파는 순방향의 신호 값을 저장할 필요가 있음.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 5.3.3 사과 쇼핑의 예  \n",
    "\n",
    "* 사과 가격에 대한 지불 금액의 미분 : 2.2\n",
    "* 사과 개수에 대한 지불 금액의 미분 : 110\n",
    "* 소비세에 대한 지불 금액의 미분    : 1.1\n",
    "    * 단위가 다르므로 주의.\n",
    "\n",
    "<center>**Figure 5-14 사과 쇼핑의 역전파 예**</center>\n",
    "![Figure_5-14](./images/Figure_5-14.png)\n",
    "\n",
    "<center>**Figure 5-15 사과와 귤 쇼핑의 역전파 예 : 빈 상자 안에 적절한 숫자를 넣어 역전파를 완성하시오.**</center>\n",
    "![Figure_5-15](./images/Figure_5-15.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## 5.4 단순한 계층 구현하기  \n",
    "\n",
    "* 구현\n",
    "    * 곱셈 노드(MulLayer)\n",
    "    * 덧셈 노드(AddLayer)\n",
    "\n",
    "* Layer 구현의 공통점\n",
    "    * forward() method : forward propagation\n",
    "    * backward() method : backward propagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 5.4.1 곱셈 계층  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "class MulLayer:\n",
    "    def __init__(self):\n",
    "        self.x = None\n",
    "        self.y = None\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        out = x * y\n",
    "        \n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        dx = dout * self.y # x와 y를 바꾼다.\n",
    "        dy = dout * self.x\n",
    "        \n",
    "        return dx, dy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "\\_\\_init\\_\\_()에서는 인스턴스 변수 x와 y를 초기화.\n",
    "이 변수들은 순전파 시의 입력 값을 유지하기 위해 필요.\n",
    "backward() 실행시 상류에서 넘어온 미분(dout)에 forward()시의 입력 값을 **서로 바꿔** 곱한 후 하류로 전달.\n",
    "\n",
    "<center>**Figure 5-16 사과 2개 구입**</center>\n",
    "![Figure_5-16](./images/Figure_5-16.png)\n",
    "\n",
    "곱셈 계층으로 구현된 class MulLayer를 이용하여 순전파(forward())를 수행시켜 볼 수 있음.\n",
    "\n",
    "**Figure 5-16**을 python으로 구현하면 다음과 같음."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Price :  220\n"
     ]
    }
   ],
   "source": [
    "apple = 100\n",
    "apple_num = 2\n",
    "tax = 1.1\n",
    "\n",
    "# 계층들\n",
    "mul_apple_layer = MulLayer()\n",
    "mul_tax_layer = MulLayer()\n",
    "\n",
    "# 순전파\n",
    "apple_price = mul_apple_layer.forward(apple, apple_num)\n",
    "price = mul_tax_layer.forward(apple_price, tax)\n",
    "\n",
    "print(\"Price : \", int(price)) # 220"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "각 변수에 대한 미분은 역전파(backward())를 통해 구할 수 있음."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dApple :  2.2\n",
      "dApple_num :  110\n",
      "dTax :  200\n"
     ]
    }
   ],
   "source": [
    "# 역전파\n",
    "dprice = 1\n",
    "dapple_price, dtax = mul_tax_layer.backward(dprice)\n",
    "dapple, dapple_num = mul_apple_layer.backward(dapple_price)\n",
    "\n",
    "# print(dapple, dapple_num, dtax) # 2.2 110 200\n",
    "print(\"dApple : \", dapple)\n",
    "print(\"dApple_num : \", int(dapple_num))\n",
    "print(\"dTax : \", dtax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "backward() 호출 순서는 forward() 때와는 반대 순서.  \n",
    "backward()가 전달 받는 인수는 **순전파의 추력에 대한 미분**임.(이 책에서는 미분값을 저장하는 변수에 **d**를 붙여 구분해 줌.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 5.4.2 덧셈 계층  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "class AddLayer:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def forward(self, x, y):\n",
    "        out = x + y\n",
    "        \n",
    "        return out\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        dx = dout * 1\n",
    "        dy = dout * 1\n",
    "        \n",
    "        return dx, dy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "덧셈 계층에서는 역전파시 순전파시의 인수를 사용하지 않으므로, \\_\\_init\\_\\_()에서는 아무 일도 하지 않습니다.(**pass**)  \n",
    "\n",
    "* 순전파(forward propagation)\n",
    "    * 입력받은 두 인수를 더해서 전달.\n",
    "* 역전파(backward propagation)\n",
    "    * 상류에서 전달 받은 미분(dout)을 그대로 하류로 전달.\n",
    "\n",
    "<center>**Figure 5-17 사과 2개와 귤 3개 구입**</center>\n",
    "![Figure_5-17](./images/Figure_5-17.png)  \n",
    "\n",
    "**Figure 5-17**을 python으로 구현하면 다음과 같음."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Price :  715\n",
      "dApple_num :  110\n",
      "dApple :  2.2\n",
      "dOrange :  3.3\n",
      "dOrange_num :  165\n",
      "dTax :  650\n"
     ]
    }
   ],
   "source": [
    "apple = 100\n",
    "apple_num = 2\n",
    "orange = 150\n",
    "orange_num = 3\n",
    "tax = 1.1\n",
    "\n",
    "# Layers\n",
    "mul_apple_layer = MulLayer()\n",
    "mul_orange_layer = MulLayer()\n",
    "add_apple_orange_layer = AddLayer()\n",
    "mul_tax_layer = MulLayer()\n",
    "\n",
    "# Forward Propagration\n",
    "apple_price = mul_apple_layer.forward(apple, apple_num) # (1)\n",
    "orange_price = mul_orange_layer.forward(orange, orange_num) # (2)\n",
    "all_price = add_apple_orange_layer.forward(apple_price, orange_price) # (3)\n",
    "price = mul_tax_layer.forward(all_price, tax) # (4)\n",
    "\n",
    "print(\"Price : \", int(price))\n",
    "\n",
    "# Backward Propagation\n",
    "dprice = 1\n",
    "dall_price, dtax = mul_tax_layer.backward(dprice) # (4)\n",
    "dapple_price, dorange_price = add_apple_orange_layer.backward(dall_price) # (3)\n",
    "dorange, dorange_num = mul_orange_layer.backward(dapple_price) # (2)\n",
    "dapple, dapple_num = mul_apple_layer.backward(dapple_price) # (1)\n",
    "\n",
    "# print(price) # 715\n",
    "# print(dapple_num, dapple, dorange, dorange_num, dtax) # 110 2.2 3.3 165 650\n",
    "print(\"dApple_num : \", int(dapple_num))\n",
    "print(\"dApple : \", dapple)\n",
    "print(\"dOrange : \", round(dorange, 1))\n",
    "print(\"dOrange_num : \", int(dorange_num))\n",
    "print(\"dTax : \", dtax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## 5.5 활성화 함수 계층 구현하기  \n",
    "\n",
    "* 활성화 함수 (Activation Function)\n",
    "    * ReLU (Rectified Linear Unit)\n",
    "    * Sigmoid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 5.5.1 ReLU 계층  \n",
    "\n",
    "활성화 함수로 사용되는 ReLU는 **Equation 5.7**과 같이 표현.\n",
    "\n",
    "$$\n",
    "\\mathbf{Equation \\ 5.7} \\\\\n",
    "y = \n",
    "\\begin{cases}\n",
    "x & (x > 0) \\\\\n",
    "0 & (x \\le 0)\n",
    "\\end{cases}\n",
    "$$  \n",
    "\n",
    "**Equation 5.7**에서 $x$에 대한 $y$의 미분은 **Equation 5.8**처럼 구할 수 있음.  \n",
    "\n",
    "$$\n",
    "\\mathbf{Equation \\ 5.8} \\\\\n",
    "\\frac{\\partial y}{\\partial x} = \n",
    "\\begin{cases}\n",
    "1 & (x > 0) \\\\\n",
    "0 & (x \\le 0)\n",
    "\\end{cases}\n",
    "$$  \n",
    "\n",
    "**Equation 5.8**에서와 같이 순전파 때의 입력 $x$가 0보다 크면 역전파는 상류의 값을 그대로 하류로 전달.\n",
    "반면, 순전파 때 이벼력 $x$가 0 이하면 역전파 때는 하류로 신호를 전달하지 않음.(0을 보냄)  \n",
    "\n",
    "계산 그래프로는 **Figure 5-18**처럼 표현.  \n",
    "\n",
    "<center>**Figure 5-18 ReLU 계층의 계산 그래프**</center>\n",
    "![Figure_5-18](./images/Figure_5-18.png)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "class Relu:\n",
    "    def __init__(self):\n",
    "        self.mask = None\n",
    "    \n",
    "    def forward(self, x):\n",
    "        self.mask = (x <= 0)\n",
    "        out = x.copy()\n",
    "        out[self.mask] = 0\n",
    "        \n",
    "        return out\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        dout[self.mask] = 0\n",
    "        dx = dout\n",
    "        \n",
    "        return dx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Relu 클래스는 \\_\\_init\\_\\_()에 mask라는 인스턴스 변수를 가짐.  \n",
    "mask는 Treu/False로 구성된 numpy array로, 순전파의 입력인 x의 원소 값이 0이하인 인덱스는 True, 그 외(0보다 큰 원소)는 False로 유지.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.  -0.5]\n",
      " [-2.   3. ]] \n",
      "\n",
      "[[False  True]\n",
      " [ True False]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "x = np.array( [[1.0, -0.5], [-2.0, 3.0]])\n",
    "print(x, \"\\n\")\n",
    "\n",
    "mask = (x <= 0)\n",
    "print(mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "**Figure 5-18**과 같이 순전파 때의 입력 값이 0이하면 역전파 때의 값은 0이 되어야 함.  \n",
    "그래서, 순전파때 만들어 놓은 mask의 원소가 True인 원소는 dout을 0으로 변경.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 5.5.2 Sigmoid 계층  \n",
    "\n",
    "활성화 함수로 사용되는 시그모이드 함수는 **Equation 5.9**와 같이 표현.\n",
    "\n",
    "$$\n",
    "\\mathbf{Equation \\ 5.9} \\\\\n",
    "y = \\frac{1}{1 + exp(-x)}\n",
    "$$  \n",
    "\n",
    "**Equation 5.9**를 계산 그래프로 그리면 **Figure 5-19**처럼 표현.  \n",
    "\n",
    "<center>**Figure 5-19 Sigmoid 계층의 계산 그래프(순전파)**</center>\n",
    "![Figure_5-19](./images/Figure_5-19.png)  \n",
    "\n",
    "**Figure 5-19**에서는 '**$\\times$**'와 '**$+$**'말고도 '**$exp$**' 와 '**$/$**' 노드가 새롭게 추가.  \n",
    "'**$exp$**'노드는 $y = exp(x)$ 계산을 수행하고 '**$/$**'노드는 $y = \\frac{1}{x}$ 계산을 수행.  \n",
    "**Figure 5-19**와 같이 **Equation 5.9**의 계산은 국소적 계산의 전파로 이루어 짐.  \n",
    "\n",
    "#### 1단계  \n",
    "\n",
    "$/$노드, 즉 $y = \\frac{1}{x}$을 미분하면 **Equation 5.10**과 같이 됨.\n",
    "\n",
    "$$\n",
    "\\mathbf{Equation \\ 5.10} \\\\\n",
    "\\begin{align} \n",
    "\\frac{\\partial y}{\\partial x} & = -\\frac{1}{x^2} \\\\ \n",
    "& = -y^2\n",
    "\\end{align} \n",
    "$$\n",
    "\n",
    "**Equation 5.10**의 계산 그래프는 아래와 같음.  \n",
    "\n",
    "![Figure_5-19-1](./images/Figure_5-19-1.png)  \n",
    "\n",
    "#### 2단계  \n",
    "'$+$'노드는 상류의 값을 그대로 하류로 보냄.  \n",
    "![Figure_5-19-2](./images/Figure_5-19-2.png)  \n",
    "\n",
    "#### 3단계  \n",
    "'**$exp$**'노드는 $y = exp(x)$ 계산을 수행하고, 그 미분은 **Equation 5.11**과 같음.  \n",
    "$$\n",
    "\\mathbf{Equation \\ 5.11} \\\\\n",
    "\\frac{\\partial y}{\\partial x} = exp(x)\n",
    "$$  \n",
    "계산 그래프에서는 상류의 값에 순전파 때의 출력($exp(-x)$)을 곱해서 하류로 전달.  \n",
    "![Figure_5-19-3](./images/Figure_5-19-3.png)  \n",
    "\n",
    "#### 4단계  \n",
    "'$\\times$'노드는 순전파 때 값을 **서로 바꿔** 곱해서 전달.  \n",
    "이 예에서는 $-1$을 곱함.  \n",
    "\n",
    "<center>**Figure 5-20 Sigmoid 계층의 계산 그래프**</center>\n",
    "![Figure_5-20](./images/Figure_5-20.png)  \n",
    "\n",
    "**Figure 5-20**에서 보면 역전파의 최종 출력인 $\\frac{\\partial L}{\\partial y} y^2 exp(-x)$값이 하류 노드로 전파됨.  \n",
    "$\\frac{\\partial L}{\\partial y} y^2 exp(-x)$는 순전파의 입력 $x$와 출력 $y$만으로 계산 가능함이 확인 됨.  \n",
    "**Figure 5-20**의 계산 그래프의 중간 과정을 생략하여 **Figure 5-21**처럼 단순한 '$Sigmoid$'노드 하나로 대체 가능.  \n",
    "\n",
    "<center>**Figure 5-21 Sigmoid 계층의 계산 그래프(간소화 버전)**</center>\n",
    "![Figure_5-21](./images/Figure_5-21.png)  \n",
    "\n",
    "**Figure 5-21**은 중간 계산을 생략할 수 있어 **Figure 5-20**에 비해 더 효율적인 계산임.  \n",
    "'\n",
    "또한, $\\frac{\\partial L}{\\partial y} y^2 exp(-x)$는 다음처럼 정리해서 표현 가능.  \n",
    "\n",
    "$$\n",
    "\\mathbf{Equation \\ 5.12} \\\\\n",
    "\\begin{align} \n",
    "\\frac{\\partial L}{\\partial y} y^2 exp(-x) & = \\frac{\\partial L}{\\partial y} \\frac{1}{(1 + exp(-x))^2} exp(-x) \\\\\n",
    "& = \\frac{\\partial L}{\\partial y} \\frac{1}{1 + exp(-x)} \\frac{exp(-x)}{1 + exp(-x)} \\\\\n",
    "& = \\frac{\\partial L}{\\partial y} y (1 - y)\n",
    "\\end{align} \n",
    "$$  \n",
    "\n",
    "이처럼 $Sigmod$ 계층의 역전파는 순전파의 출력($y$)만으로 계산 가능함.  \n",
    "\n",
    "<center>**Figure 5-22 Sigmoid 계층의 그래프 : 순전파의 출력 $y$만으로 역전파를 계산할 수 있다.**</center>\n",
    "![Figure_5-22](./images/Figure_5-22.png)  \n",
    "\n",
    "$Sigmoid$ 계층을 python으로 구현하면 아래와 같음.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "class Sigmoid:\n",
    "    def __init__(self):\n",
    "        self.out = None\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = 1 / (1 + np.exp(-x))\n",
    "        self.out = out\n",
    "        \n",
    "        return out\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        dx = dout * (1.0 - self.out) * self.out\n",
    "        \n",
    "        return dx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "이 구현에서는 순전파의 출력을 인스턴스 변수 out에 보관했다가, 역전파 때 그 변수를 사용해 계산을 수행.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## 5.6 Affine/Softmax 계층 구현하기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 5.6.1 Affine 계층  \n",
    "\n",
    "* 신경망의 순전파 때 수행하는 행렬의 내적은 기하학에서 **어파인 변환(Affine transfomation)**이라고 함."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2,)\n",
      "(2, 3)\n",
      "(3,)\n",
      "[ 0.79093848  0.76577485  0.89965323]\n"
     ]
    }
   ],
   "source": [
    "X = np.random.rand(2)    # Input\n",
    "W = np.random.rand(2, 3) # Weight\n",
    "B = np.random.rand(3)    # Bias\n",
    "\n",
    "print(X.shape) # (2,)\n",
    "print(W.shape) # (2, 3)\n",
    "print(B.shape) # (3,)\n",
    "\n",
    "Y = np.dot(X, W) + B\n",
    "print(Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "**X**, **W**, **B**는 형상이 (2,), (2, 3), (3,)인 다차원 배열.  \n",
    "그러면 뉴런의 가중치 합은 **Y = np.dot(X, W) + B** 처럼 계산하여 **Y**를 활성화 함수로 변환하여 다음 층으로 전파하는 것이 신경망 순전파의 흐름.  \n",
    "(이때 행렬의 내적 계산은 대응하는 차원의 원소 수를 일치시키는 것이 핵심.)  \n",
    "\n",
    "<center>**Figure 5-23 행렬의 내적에서는 대응하는 차원의 원소 수를 일치시킨다.**</center>\n",
    "![Figure_5-23](./images/Figure_5-23.png)  \n",
    "\n",
    "내적을 계산 하는 노드를 '$dot$'이라 하면, **np.dot(X, W) + B** 계산은 **Figure 5-24**처럼 표현.  \n",
    "\n",
    "<center>**Figure 5-24 Affine 계층의 계산 그래프 : 변수가 행렬임에 주의. 각 변수의 형상을 변수명 위에 표기했다.**</center>\n",
    "![Figure_5-24](./images/Figure_5-24.png)  \n",
    "\n",
    "**Figure 5-24**는 비교적 단순한 계산 그래프이지만, 단. **X**, **W**, **B**가 행렬(다차원 배열)이라는 점에 주의해야 함.  \n",
    "**Figure 5-24**의 행렬을 사용한 역전파도 행렬의 원소마다 전개해 보면 스칼라값을 사용한 지금까지의 계산 그래프와 같은 순서로 생각할 수 있음.  \n",
    "전개해 보면 **Equation 5.13**과 같이 표현.  \n",
    "\n",
    "$$\n",
    "\\mathbf{Equation \\ 5.13} \\\\\n",
    "\\frac{\\partial L}{\\partial \\boldsymbol{X}} = \\frac{\\partial L}{\\partial \\boldsymbol{Y}} \\centerdot \\boldsymbol{W}^T \\\\\n",
    "\\frac{\\partial L}{\\partial \\boldsymbol{W}} = \\boldsymbol{X}^T \\centerdot \\frac{\\partial L}{\\partial \\boldsymbol{Y}}\n",
    "$$  \n",
    "\n",
    "**Equation 5.13**에서의 $\\boldsymbol{W}^T$의 $T$는 전치행렬을 뜻함.  \n",
    "전치행렬은 $\\boldsymbol{W}$의 $(i, j)$ 위치의 원소를  $(j, i)$ 위치로 바꾼 것을 뜻함.  \n",
    "수식으로는 **Equation 5.14** 같이 표현. \n",
    "\n",
    "$$\n",
    "\\mathbf{Equation \\ 5.14} \\\\\n",
    "\\begin{align}\n",
    "& \\boldsymbol{W} = \\begin{pmatrix} w_{11} & w_{21} & w_{31} \\\\ w_{12} & w_{22} & w_{32} \\end{pmatrix} \\\\\n",
    "& \\boldsymbol{W}^T = \\begin{pmatrix} w_{11} & w_{12} \\\\ w_{21} & w_{22} \\\\ w_{31} & w_{32} \\end{pmatrix}\n",
    "\\end{align}\n",
    "$$  \n",
    "\n",
    "**Equation 5.14**와 같이 $\\boldsymbol{W}$의 형상이 $(2, 3)$이었다면 전치행렬 $\\boldsymbol{W}^T$의 형상은 $(3, 2)$가 됨.  \n",
    "**Eqaution 5.13**을 바탕으로 계산 그래프의 역전파를 구해 보면 **Figure 5-25**처럼 표현.  \n",
    "\n",
    "<center>**Figure 5-25 Affine 계층의 역전파 : 변수가 다차원 배열임에 주의. 역전파에서의 변수 형상은 해당 변수명 아래에 표기했다.**</center>\n",
    "![Figure_5-25](./images/Figure_5-25.png)  \n",
    "\n",
    "**Figure 5-25**의 계산 그래프에서는 각 변수의 형상에 주의.  \n",
    "$\\boldsymbol{X}$와 $\\frac{\\partial L}{\\partial \\boldsymbol{X}}$은 같은 형상이고, $\\boldsymbol{W}$와 $\\frac{\\partial L}{\\partial \\boldsymbol{W}}$도 같은 형상임.  \n",
    "$\\boldsymbol{X}$와 $\\frac{\\partial L}{\\partial \\boldsymbol{X}}$이 같다는 것은 **Equation 5.15**를 보면 명확해짐.  \n",
    "\n",
    "$$\n",
    "\\mathbf{Equation \\ 5.15} \\\\\n",
    "\\begin{align}\n",
    "& \\boldsymbol{X} = (x_0, x_1, \\cdots, x_n) \\\\\n",
    "& \\frac{\\partial z}{\\partial \\boldsymbol{X}} = \\left( \\frac{\\partial L}{\\partial x_0}, \\frac{\\partial L}{\\partial x_1}, \\cdots, \\frac{\\partial L}{\\partial x_n}, \\right)\n",
    "\\end{align}\n",
    "$$  \n",
    "\n",
    "$\\frac{\\partial L}{\\partial \\boldsymbol{Y}}$의 형상이 $(3,)$ $\\boldsymbol{W}$의 형상이 $(2, 3)$일 때, $\\frac{\\partial L}{\\partial \\boldsymbol{X}}$의 형상이 $(2,)$가 되도록 하는 $\\frac{\\partial L}{\\partial \\boldsymbol{Y}}$ 과 $\\boldsymbol{W}$의 내적은 **Figure 5-26**에서 확인할 수 있음.  \n",
    "(**Equation 5.13** 참고)\n",
    "\n",
    "<center>**Figure 5-26 행렬 내적('dot'노드)의 역전파는 행렬의 대응하는 차원의 원소 수가 일치하도록 내적을 조립하여 구할 수 있다.**</center>\n",
    "![Figure_5-26](./images/Figure_5-26.png)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 5.6.2 배치용 Affine 계층  \n",
    "\n",
    "이전의 Affine 계층은 입력 데이터로 $\\boldsymbol{X}$ 하나만을 고려하였음.  \n",
    "이제 배치용 Affine 계층을 고려.  \n",
    "배치용 Affine 계층을 계산 그래프로 표현 하면 **Figure 5-27**과 같음.  \n",
    "\n",
    "<center>**Figure 5-27 배치용 Affine 계층의 계산 그래프**</center>\n",
    "![Figure_5-27](./images/Figure_5-27.png)    \n",
    "\n",
    "이전과 다른 부분은 입력인 $\\boldsymbol{X}$의 형상이 $(N, 2)$가 된 것 뿐임.  \n",
    "역전파 때는 행렬의 형상에 주의 하면  $\\frac{\\partial L}{\\partial \\boldsymbol{X}}$과 $\\frac{\\partial L}{\\partial \\boldsymbol{W}}$은 이전과 같이 도출 가능.  \n",
    "편향을 더할 때도 주의해야 함. 편향은 N개 Row에 Broadcasting하여 더해짐.  \n",
    "아래 예로 편향의 계산을 확인 가능.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0  0  0]\n",
      " [10 10 10]] \n",
      "\n",
      "[[ 1  2  3]\n",
      " [11 12 13]]\n"
     ]
    }
   ],
   "source": [
    "X_dot_W = np.array( [[0, 0, 0], [10, 10, 10]] )\n",
    "B = np.array( [1, 2, 3] )\n",
    "\n",
    "print(X_dot_W, \"\\n\")\n",
    "print(X_dot_W + B)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "순전파의 편향 덧셈은 각각의 데이터(1번째 데이터, 2번째 데이터, ...)에 더해짐.  \n",
    "역전파 때는 형상을 맞춰야 하므로 데이터의 역전파 값이 편향의 원소에 모여야 함.  \n",
    "코드로는 아래와 같음.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 2 3]\n",
      " [4 5 6]] \n",
      "\n",
      "[5 7 9]\n"
     ]
    }
   ],
   "source": [
    "dY = np.array( [[1, 2, 3], [4, 5, 6]] )\n",
    "print(dY, \"\\n\")\n",
    "\n",
    "dB = np.sum(dY, axis = 0) # axis = 0은 row 기준.\n",
    "print(dB)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "Affine 계층 구현을 python으로 하면 아래와 같음.  \n",
    "(common/layers.py의 구현은 데이터가 4차원인 경우도 고려한 것이라 아래의 구현과는 약간 차이가 존재.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "class Affine:\n",
    "    def __init__(self, W, b):\n",
    "        self.W = W\n",
    "        self.b = b\n",
    "        self.x = None\n",
    "        self.dW = None\n",
    "        self.db = None\n",
    "        \n",
    "    def forward(self, x):\n",
    "        self.x = x\n",
    "        out = np.dot(x, self.W) + self.b\n",
    "        \n",
    "        return out\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        dx = np.dot(dout, self.W.T)\n",
    "        self.dW = np.dot(self.x.T, dout)\n",
    "        self.db = np.sum(dout, axis = 0)\n",
    "        \n",
    "        return dx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 5.6.3 Softmax-with-Loss 계층  \n",
    "\n",
    "* 소프트맥스 함수는 입력 값을 정규화하여 출력.  \n",
    "    * 출력값의 합이 1이 되도록.  \n",
    "\n",
    "손글씨 숫자 인식에서의 Softmax 계층의 출력은 **Frigure 5-28**과 같음.  \n",
    "\n",
    "<center>**Figure 5-28 입력 이미지가 Affine 계층과 ReLU 계층을 통과하며 변환되고, 마지막 Softmax 계층에 의해서 10개의 입력이 정규화된다.  \n",
    "이 그림에서는 숫자 '0'의 점수는 5.3이며, 이것이 Softmax 계층에 의해서 0.008(0.8%)로 변환된다. 또, '2'의 점수는 10.1에서 0.991(99.1%)로 변환된다.**</center>\n",
    "![Figure_5-28](./images/Figure_5-28.png)  \n",
    "\n",
    "**Figure 5-28**과 같이 Softmax 계층은 입력 밧을 정규화(출력의 합이 1이 되도록 변형)하여 출력.  \n",
    "손글씨 숫자는 가짓수가 10개(10클래스 분류)이므로 Softmax 계층의 입력과 출력은 10개가 됨.  \n",
    "\n",
    "* 신경망 추론에서는 답을 하나만 내는 경우에는 가장 높은 점수만 알면 되니 Softmax 계층은 사용하지 않음.\n",
    "* 반면, 신경망을 학습할때는 Softmax 계층이 필요.\n",
    "\n",
    "소프트맥스 계층을 구현할 때, 손실 함수인 교차 엔트로피 오차도 포함하여 **Softmax-with-Loss 계층**이라는 이름으로 구현.  \n",
    "**Figure 5-29**에서 Softmax-with-Loss 계층의 계산 그래프를 확인.  \n",
    "\n",
    "<center>**Figure 5-29 Softmax-with-Loss 계층의 계산 그래프**</center>\n",
    "![Figure_5-29](./images/Figure_5-29.png)  \n",
    "\n",
    "Softmax-with-Loss 계층은 다소 복잡하나, **Figure 5-30**의 계산 그래프와 같이 간소화할 수 있음.  \n",
    "\n",
    "<center>**Figure 5-30 '간소화한' Softmax-with-Loss 계층의 계산 그래프**</center>\n",
    "![Figure_5-30](./images/Figure_5-30.png)  \n",
    "\n",
    "예시)  \n",
    "$t = (0, 1, 0)$이고 Softmax 계층이 $(0.3, 0.2, 0.5)$출력했다고 가정 하면, Softmax 계층의 역전파는 $(0.3, -0.8, 0.5)$로 커다란 오차를 전파.  \n",
    "$t = (0, 1, 0)$이고 Softmax 계층이 $(0.01, 0.99, 0)$출력했다고 가정 하면, Softmax 계층의 역전파는 $(0.01, -0.01, 0)$로 작은 오차를 전파. \n",
    "\n",
    "Softmax-with-Loss 계층을 python으로 구현하면 아래와 같음.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "class SoftWithLoss:\n",
    "    def __init_(self):\n",
    "        self.loss = None\n",
    "        self.y = None\n",
    "        self.t = None\n",
    "        \n",
    "    def forward(self, x, t):\n",
    "        self.t = t\n",
    "        self.y = softmax(x)\n",
    "        self.loss = cross_entropy_enter(self.y, self.t)\n",
    "        \n",
    "        return self.loss\n",
    "    \n",
    "    def backward(self, dout = 1):\n",
    "        batch_size = self.shape[0]\n",
    "        dx = (self.y - self.t) / batch_size # Batch size로 나눈 것에 주의. \n",
    "        \n",
    "        return dx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## 5.7 오차역전파법 구현하기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 5.7.1 신경망 학습의 전체 그림  \n",
    "\n",
    "* **전제**\n",
    "    * 신경망에는 적응 가능한 가중치와 편향이 있고, 이 가중치와 편향을 훈련 데이터에 적용하도록 조정하는 과정을 **학습**이라고 함.\n",
    "    * 신경망 학습은 아래 4단계로 수행.\n",
    "* **1단계 - 미니배치**\n",
    "    * 훈련 데이터 중 일부를 무작위로 가져옴. 이렇게 선별한 데이터를 미니배치라 하며, 그 미니배치의 손실 함수 값을 줄이는 것을 목표로 함.\n",
    "* **2단계 - 기울기 산출**\n",
    "    * 미니배치의 손실 함수 값을 줄이기 위해 각 가중치 매개변수의 기울기를 구함.\n",
    "    * 기울기는 손실 함수의 값을 가장 작게 하는 방향을 제시.\n",
    "* **3단계 - 매개변수 갱신**\n",
    "    * 가중치 매개변수를 기울기 방향으로 아주 조금 갱신.\n",
    "* **4단계 - 반복**\n",
    "    * 1~3단계를 반복.\n",
    "\n",
    "오차역전파법이 등장하는 단계는 두번째인 **기울기 산출**.  \n",
    "앞 장에서 배운 수치 미분은 구현은 쉽지만 비효율적임.  \n",
    "오차역전파법을 이용하면 느린 수치 미분과 달리 기울기를 효율적이고 빠르게 구할 수 있음.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 5.7.2 오차역전파법을 적용한 신경망 구현하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import sys, os\n",
    "# sys.pasth.append(os.pardir)\n",
    "import numpy as np\n",
    "from common.layers import *\n",
    "from common.gradient import numerical_gradient\n",
    "from collections import OrderedDict\n",
    "\n",
    "class TwoLayerNet:\n",
    "    def __init__(self, input_size, hidden_size, output_size, weight_init_std = 0.01):\n",
    "        # initial weigths\n",
    "        self.params = {}\n",
    "        self.params['W1'] = weight_init_std * np.random.randn(input_size, hidden_size)\n",
    "        self.params['b1'] = np.zeros(hidden_size)\n",
    "        self.params['W2'] = weight_init_std * np.random.randn(hidden_size, output_size)\n",
    "        self.params['b2'] = np.zeros(output_size)\n",
    "        \n",
    "        # create layers                                                       # Important\n",
    "        self.layers = OrderedDict()                                           # Important\n",
    "        self.layers['Affine1'] = Affine(self.params['W1'], self.params['b1']) # Important\n",
    "        self.layers['Relu1'] = Relu()                                         # Important\n",
    "        self.layers['Affine2'] = Affine(self.params['W2'], self.params['b2']) # Important\n",
    "        self.lastLayer = SoftmaxWithLoss()                                    # Important\n",
    "        \n",
    "    def predict(self, x):\n",
    "        for layer in self.layers.values():                                    # Important\n",
    "            x = layer.forward(x)                                              # Important\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    # x : input data, t : true label\n",
    "    def loss(self, x, t):\n",
    "        y = self.predict(x)\n",
    "        \n",
    "        return self.lastLayer.forward(y, t)\n",
    "    \n",
    "    def accuracy(self, x, t):\n",
    "        y = self.predict(x)\n",
    "        y = np.argmax(y, axis = 1)\n",
    "        if t.ndim != 1:\n",
    "            t = np.argmax(t, axis = 1)\n",
    "        \n",
    "        accuracy = np.sum(y == t) / float(x.shape[0])\n",
    "        \n",
    "        return accuracy\n",
    "    \n",
    "    # x : input data, t : true label\n",
    "    def numerical_gradient(self, x, t):\n",
    "        loss_W = lambda W: self.loss(x, t)\n",
    "        \n",
    "        grads = {}\n",
    "        grads['W1'] = numerical_gradient(loss_W, self.params['W1'])\n",
    "        grads['b1'] = numerical_gradient(loss_W, self.params['b1'])\n",
    "        grads['W2'] = numerical_gradient(loss_W, self.params['W2'])\n",
    "        grads['b2'] = numerical_gradient(loss_W, self.params['b2'])\n",
    "        \n",
    "        return grads\n",
    "    \n",
    "    def gradient(self, x, t):\n",
    "        # forward propagation\n",
    "        self.loss(x, t)\n",
    "        \n",
    "        # backward propagation\n",
    "        dout = 1\n",
    "        dout = self.lastLayer.backward(dout)\n",
    "        \n",
    "        layers = list(self.layers.values())\n",
    "        layers.reverse()\n",
    "        for layer in layers:\n",
    "            dout = layer.backward(dout)\n",
    "        \n",
    "        # result save\n",
    "        grads = {}\n",
    "        grads['W1'] = self.layers['Affine1'].dW\n",
    "        grads['b1'] = self.layers['Affine1'].db\n",
    "        grads['W2'] = self.layers['Affine2'].dW\n",
    "        grads['b2'] = self.layers['Affine2'].db\n",
    "        \n",
    "        return grads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "**신경망 계층을 OrderedDict에 보관하는 점이 중요**\n",
    "OrderedDict는 순서가 있는 딕셔너리.  \n",
    "**순서가 있는**이란 딕서너리에 추가한 순서를 기억한다는 것.  \n",
    "\n",
    "* 순전파 때는 순서대로 각 계층의 forward() 메서드를 호출하면 되고, 역전파 때는 역순으로 backward() 메서드를 호출해주면 됨.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 5.7.3 오차역전파법으로 구한 기울기 검증하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b2:1.19459991899e-10\n",
      "W2:7.16269371939e-13\n",
      "b1:7.71072048645e-13\n",
      "W1:2.08732567469e-13\n"
     ]
    }
   ],
   "source": [
    "import sys, os\n",
    "# sys.path.append(os.pardir)\n",
    "import numpy as np\n",
    "from dataset.mnist import load_mnist\n",
    "#from two_layer_net import TwoL1ayerNet\n",
    "\n",
    "# load data\n",
    "(x_train, t_train), (x_test, t_test) = load_mnist(normalize = True, one_hot_label = True)\n",
    "\n",
    "network = TwoLayerNet(input_size = 784, hidden_size = 50, output_size = 10)\n",
    "\n",
    "x_batch = x_train[:3]\n",
    "t_batch = t_train[:3]\n",
    "\n",
    "grad_numerical = network.numerical_gradient(x_batch, t_batch)\n",
    "grad_backprop = network.gradient(x_batch, t_batch)\n",
    "\n",
    "# calc abs error per weight\n",
    "for key in grad_numerical.keys():\n",
    "    diff = np.average(np.abs(grad_backprop[key] - grad_numerical[key]))\n",
    "    print(key + \":\" + str(diff))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 5.7.4 오차역전파법을 사용한 학습 구현하기  \n",
    "\n",
    "수치 미분의 이점은 구현하기 쉽고, 버그가 존재하기 힘드므로 실수하기 쉬운 오차역전파법 구현 검증에 사용.  \n",
    "\n",
    "두 방식으로 기울기가 일치함(거의 같음)을 확인하는 작업을 **기울기 확인(gradient check)**라고 함.  \n",
    "두 방식의 오차가 아주 작으면 정상.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'TwoLayerNet' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-78af2204424a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt_test\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_mnist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnormalize\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mone_hot_label\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mnetwork\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTwoLayerNet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m784\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0miters_num\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10000\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'TwoLayerNet' is not defined"
     ]
    }
   ],
   "source": [
    "import sys, os\n",
    "#sys.path.append(os.pardir)\n",
    "import numpy as np\n",
    "from dataset.mnist import load_mnist\n",
    "#from two_layer_net import TwoL1ayerNet\n",
    "\n",
    "# load data\n",
    "(x_train, t_train), (x_test, t_test) = load_mnist(normalize = True, one_hot_label = True)\n",
    "\n",
    "network = TwoLayerNet(input_size = 784, hidden_size = 50, output_size = 10)\n",
    "\n",
    "iters_num = 10000\n",
    "train_size = x_train.shape[0]\n",
    "batch_size = 100\n",
    "learning_rate = 0.1\n",
    "\n",
    "train_loss_list = []\n",
    "train_acc_list = []\n",
    "test_acc_list = []\n",
    "\n",
    "iter_per_epoch = max(train_size / batch_size, 1)\n",
    "for i in range(iters_num):\n",
    "    batch_mask = np.random.choice(train_size, batch_size)\n",
    "    x_batch = x_train[batch_mask]\n",
    "    t_batch = t_train[batch_mask]\n",
    "    \n",
    "    # calculate gradient with backpropagation\n",
    "    grad = network.gradient(x_batch, t_batch)\n",
    "    \n",
    "    # \n",
    "    for key in ('W1', 'b1', 'W2', 'b2'):\n",
    "        network.params[key] -= learning_rate * grad[key]\n",
    "    \n",
    "    loss = network.loss(x_batch, t_batch)\n",
    "    train_loss_list.append(loss)\n",
    "    \n",
    "    if i % iter_per_epoch == 0:\n",
    "        train_acc = network.accuracy(x_train, t_train)\n",
    "        test_acc = network.accuracy(x_test, t_test)\n",
    "        train_acc_list.append(train_acc)\n",
    "        test_acc_list.append(test_acc)\n",
    "        print(\"Train Accuracy : \", round(train_acc, 4), \"Test Accuracy : \", test_acc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\frac{b}{a}\n",
    "$$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
